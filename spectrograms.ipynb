{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO5w97JrCOOR5tB3eTqQqpX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elijkon/DeepLearning_MiniHackathon/blob/main/spectrograms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-t_W--_bWIN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
        "\n",
        "Filename identifiers\n",
        "\n",
        "1) Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "\n",
        "2) Vocal channel (01 = speech, 02 = song).\n",
        "\n",
        "3) Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "\n",
        "4) Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "\n",
        "5) Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "\n",
        "6) Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "\n",
        "7) Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 03-01-06-01-02-01-12.wav\n",
        "\n",
        "Audio-only (03)\n",
        "Speech (01)\n",
        "Fearful (06)\n",
        "Normal intensity (01)\n",
        "Statement \"dogs\" (02)\n",
        "1st Repetition (01)\n",
        "12th Actor (12)\n",
        "Female, as the actor ID number is even.\n",
        "How to cite the RAVDESS\n",
        "\n",
        "Academic citation\n",
        "If you use the RAVDESS in an academic publication, please use the following citation: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and\n",
        "Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "All other attributions\n",
        "If you use the RAVDESS in a form other than an academic publication, such as in a blog post, school project, or non-commercial product,\n",
        "please use the following attribution: \"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\" by Livingstone & Russo is licensed under CC BY-NA-SC 4.0.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ4SbeO8e6jp",
        "outputId": "3d01d7fa-16fb-44a0-a915-e4e6966c718b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 2: Import libraries\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#\n",
        "# STEP 3: Configure paths\n",
        "input_dir = \"/content/drive/MyDrive/school classes/deep learning/data/audio_speech_actors_01-24\"  # unzipped RAVDESS folder (contains Actor_01, Actor_02, ...)\n",
        "output_dir = \"/content/drive/MyDrive/school classes/deep learning/data/spectrogram_dataset\"  # where to save spectrograms\n",
        "\n",
        "# STEP 4: Define emotion mapping\n",
        "emotion_map = {\n",
        "    1: \"neutral\",\n",
        "    2: \"calm\",\n",
        "    3: \"happy\",\n",
        "    4: \"sad\",\n",
        "    5: \"angry\",\n",
        "    6: \"fearful\",\n",
        "    7: \"disgust\",\n",
        "    8: \"surprised\"\n",
        "}\n",
        "\n",
        "\n",
        "def wav_to_melspectrogram(wav_path, save_path):\n",
        "    y, sr = librosa.load(wav_path, sr=None)  # load audio file\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)  # mel spectrogram\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # convert to decibels\n",
        "\n",
        "    # Save as image\n",
        "    plt.figure(figsize=(2.5, 2.5))\n",
        "    librosa.display.specshow(mel_spec_db, sr=sr, cmap=\"magma\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.savefig(save_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "#\n",
        "# STEP 6: Collect all wav files grouped by emotion\n",
        "#\n",
        "data_by_emotion = {emotion: [] for emotion in emotion_map.values()}\n",
        "\n",
        "# os.walk → goes into Actor_01, Actor_02, etc.\n",
        "for root, dirs, files in os.walk(input_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            parts = file.split(\"-\")                 # e.g. 03-01-06-01-02-01-12.wav\n",
        "            emotion_code = int(parts[2])            # 3rd number = emotion\n",
        "            emotion_label = emotion_map[emotion_code]\n",
        "            wav_path = os.path.join(root, file)     # full path including actor folder\n",
        "            data_by_emotion[emotion_label].append((wav_path, file))\n",
        "\n",
        "\n",
        "# STEP 7: Split into train/val/test and save\n",
        "split_ratios = {\"train\": 0.7, \"val\": 0.15, \"test\": 0.15}\n",
        "\n",
        "for emotion, files in data_by_emotion.items():\n",
        "    # Split into train and test\n",
        "    train_files, test_files = train_test_split(files, test_size=split_ratios[\"test\"], random_state=42)\n",
        "    # Split train further into train and val\n",
        "    train_files, val_files = train_test_split(\n",
        "        train_files,\n",
        "        test_size=split_ratios[\"val\"] / (1 - split_ratios[\"test\"]),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Save splits into folders\n",
        "    for split_name, split_files in zip([\"train\", \"val\", \"test\"], [train_files, val_files, test_files]):\n",
        "        split_folder = os.path.join(output_dir, split_name, emotion)\n",
        "        os.makedirs(split_folder, exist_ok=True)\n",
        "\n",
        "        for wav_path, filename in tqdm(split_files, desc=f\"{emotion} -> {split_name}\"):\n",
        "            save_path = os.path.join(split_folder, filename.replace(\".wav\", \".png\"))\n",
        "            wav_to_melspectrogram(wav_path, save_path)\n",
        "\n",
        "\n",
        "print(\"Spectrograms saved in train/val/test structure at:\", output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEG6sU8KVxXd",
        "outputId": "8675cf31-bd4d-47cb-bc24-8f9fb3fed4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "neutral -> train: 100%|██████████| 66/66 [00:55<00:00,  1.19it/s]\n",
            "neutral -> val: 100%|██████████| 15/15 [00:08<00:00,  1.71it/s]\n",
            "neutral -> test: 100%|██████████| 15/15 [00:09<00:00,  1.65it/s]\n",
            "calm -> train: 100%|██████████| 134/134 [01:23<00:00,  1.60it/s]\n",
            "calm -> val: 100%|██████████| 29/29 [00:18<00:00,  1.53it/s]\n",
            "calm -> test: 100%|██████████| 29/29 [00:17<00:00,  1.64it/s]\n",
            "happy -> train: 100%|██████████| 134/134 [01:23<00:00,  1.61it/s]\n",
            "happy -> val: 100%|██████████| 29/29 [00:18<00:00,  1.53it/s]\n",
            "happy -> test: 100%|██████████| 29/29 [00:18<00:00,  1.57it/s]\n",
            "sad -> train: 100%|██████████| 134/134 [01:20<00:00,  1.66it/s]\n",
            "sad -> val: 100%|██████████| 29/29 [00:18<00:00,  1.59it/s]\n",
            "sad -> test: 100%|██████████| 29/29 [00:18<00:00,  1.55it/s]\n",
            "angry -> train: 100%|██████████| 134/134 [01:25<00:00,  1.58it/s]\n",
            "angry -> val: 100%|██████████| 29/29 [00:18<00:00,  1.57it/s]\n",
            "angry -> test: 100%|██████████| 29/29 [00:18<00:00,  1.56it/s]\n",
            "fearful -> train: 100%|██████████| 134/134 [01:21<00:00,  1.64it/s]\n",
            "fearful -> val: 100%|██████████| 29/29 [00:18<00:00,  1.58it/s]\n",
            "fearful -> test: 100%|██████████| 29/29 [00:18<00:00,  1.57it/s]\n",
            "disgust -> train: 100%|██████████| 134/134 [01:28<00:00,  1.51it/s]\n",
            "disgust -> val: 100%|██████████| 29/29 [00:17<00:00,  1.64it/s]\n",
            "disgust -> test: 100%|██████████| 29/29 [00:17<00:00,  1.70it/s]\n",
            "surprised -> train: 100%|██████████| 134/134 [01:22<00:00,  1.63it/s]\n",
            "surprised -> val: 100%|██████████| 29/29 [00:17<00:00,  1.66it/s]\n",
            "surprised -> test: 100%|██████████| 29/29 [00:17<00:00,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! Spectrograms saved in train/val/test structure at: /content/drive/MyDrive/school classes/deep learning/data/spectrogram_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}